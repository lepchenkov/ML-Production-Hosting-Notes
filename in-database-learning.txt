Sumit Kumar 2017 Pydata Seattle talk

Bringing intelligence to where the data lives. 

The old approach is to separate the database and application/intelligence.
The new approach is to separate application and intelligence/database.

Old approach: we pull data from the database and bulid python-based model. 
And the model sits in the application. And there are issues with it. 

And the new paradigm it to move the intelligence and the models right where
the data is. And this model becomes the asset of the enterprise. And any 
number of applications can get access to this intelligence by connecting 
to this database (intelligence database).

Why do we want in database ML?
1. Eliminate data movement. So rather taking data to the compute we are 
moving compute to the data. Moving massive amounts of data is often not 
very easy to do. And many times you are just not allowed to move the data 
because of security reasons. 

2. Once the model is developed, putting model into production causes
challenges. Sometimes data sciecnce team gives model to app development 
team and the app team translates those models into the language of the 
application. And there is latency involved in it and fidelity loss. 
But if we do operationalization using SQL server what we are doing is 
embedding python code right there inside T-SQL stored procedure and the 
application just makes a T-SQL stored procedure call and gets all of this 
smarts. So from application standpoint you even might not be aware that 
there is a python code running involved. And the model becomes just a binary
data that we store inside a SQL server as well. So managing different 
verions of models becomes way more simple. 

3. The third advantage is enterprise gradeness and scale of this approach. 
We are using microsoftml package. This package offeres a number of 
parallelized and scalable algorithms. Because of that the ML solution 
become scalable. And the SQL server itself has decades of innovations. And
all of this becomes available to ML algorithms. One thing is in-memory quering
capability. If we combine this innovations with the scalalable versions of ML
alrothims then we really get a tremendous performance advantage.    

When installing the SQL server there is an option in "Feature Selection" 
section to include "Machine Learning Services (In-Database)". There we can 
choose both Python and R.

### shows the demo ###
Taking the Python code and running it inside the SQL server is as simple as that:

    execute sp_execute_external_script
    @language = N'Python',
    @script = N'
    import sys
    import os 
    print('*****')
    print('!!!Hello World!!!')
    print('*****')
    '

It a a special system stored procedure that understands the code for different
languages like Python and R. And it offers some additional tidy integration with
SQL. 

Slightly more advanced example of T-SQL stored procedure with embedded Python 
code (we are doing simple subsetting):

    execute sp_execute_external_script
    @language = N'Python',
    @script = N'
    OutputDataSet = pandas.DataFrame(InputDataSet[InputColumnName])
    OutputRowCount = len(OutputDataSet)
    ',
    @input_data_1 = N'select * from #PythonTest',
    @params = N'@InputColumnName varchar(max), @OutputRowCount INT OUTPUT',
    @InputColumnName = @InputColumnName,
    @OutputRowCount = @OutputRowCount OUTPUT

    print @OutputRowCount

We can pass any SQL query to this Python script. We can also pass a number of 
parameters.  

What the architecture looks like:
So the python and R runtimes are integrated into database in the way that it is 
not running inside a SQL process but in the database server machine. So what 
happens is: when it sees the sp_execute_external_script, the SQL process 
Launchpad.exe invokes the external runtime through appropriate launcher based on 
a parameter language. And this python process is a separate process running on 
the same machine. And also this process is runned under separate user account 
which has very low priveledge so the Python script can not to the random things. 
And when microsoft ML algorithms are used then it invokes another BxlServer.exe
process which runs all the algorithms and native C++ code. And this process has
high through put data communication channel with SQL server process. And that is
how it achieves a lot of performance scale.


revoscalepy - Python package to scale and performant alalytics.
Scalabale and parallel predictive modeling algos: Linear regression, Logistic
regresion, Decision trees, Boosted trees, Random Forest, Predictions. 
Compute contexts: Local, SQL Server.
Data sources, ETL, Utilities: ODBC Data Source, SQL Data Source, Xdf Data Source,
Data Step, Data Import, Summarization. 
There is an initiator component that takes cate of distribution the job across 
multiple different tasks. And then each task gets chunks of data from disc. 
Because of that it can work with arbitarily large datasets. And the entire thing
is written in C++ performant code so you get performance advantage. And revoscalepy
can work both locally or on a server. And this whole thing is called PEMA - 
Parallel External Memory Algorithms.

And the other library is microsoftml which is the collection of algorithms that
were used in microsoft internal products. And now microsoft makes microsoftml a 
part of SQL server.  
Learners: DNNs, Logistic Regression, One-class SVM, Fast Tree, Fast Forest, 
Fast Linear (SDCA).
It also has pre-trained models for sentiment analysis and image featurizers (
ResNet-50, AlexNet, etc.). The library has also transforms for feature selection,
transformation, etc. 

______________________
Demo of predicting length of stay in the hospital:
The business problem is that unlike staying in a hotel, in the hospital patients
don't really know for how long they are going to stay in it. So we need to 
predict it so the human resources in the hospital can be manages prorerly. 

As a data scientist you work in an IDE. And you have separate scripts: data_
processing_step_1.py, feature_engineering_stage_2.py, 
training_and_evaluation_stage_3.py and sql_connections.py.

In training_and_evaluation_stage_3 there is a line 
    RxComputeContext.rx_set_compute_context(sql)
which sets the compute context to SQL for model training. 
And in the beginning of the document the next lines are:
    connection_sring = "Driver=SQL .... ...."
    sql = RxInSqlServer.RxInSqlServer(connection_sring - connection_sring)
    local = RxLocalSeq.RxLocalSeq()  

So after the line RxComputeContext.rx_set_compute_context(sql) the subsequent
lines of code are note executed locally, their execution is happening on a SQL
server. 